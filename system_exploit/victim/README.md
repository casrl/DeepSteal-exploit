# PyTorch Inference Server

This directory contains the victim code to run continuous PyTorch inference using RESNET-18 quantized model. This is adapted from: https://pytorch.org/tutorials/advanced/cpp_export.html

## Environment
- **Operating System:** Ubuntu 20.04.4 LTS
- **Kernel:** 5.13.0-40-generic
- **GCC:** Ubuntu 9.4.0-1ubuntu1~20.04.1
- **Python:** 3.9.7 (Anaconda distribution)
- **Pytorch:** Source compiled (Tag: v1.7.1-rc3, FBGEMM commit: 1d71039)

## Setup

- Compile PyTorch from source (https://github.com/pytorch/pytorch/tree/v1.7.1-rc3)
```bash
# Install dependencies
conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses

# Download sources
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync
git submodule update --init --recursive
git checkout --recurse-submodules tags/v1.7.1-rc3
git submodule update --recursive

# Compile Pytorch
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
DEBUG=1 USE_CUDA=0 USE_CUDNN=0  USE_FBGEMM=1 python setup.py develop
```
- Compile victim application
```bash
mkdir build
cd build

cmake -DCMAKE_PREFIX_PATH=<path_to_pytorch_directory>/torch ..
make
```
